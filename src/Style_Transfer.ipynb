{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHgJ4k3PTSuQ"
      },
      "source": [
        "#Importing the required tools#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AQIQplQ8Zha"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boBTTKoyURFz"
      },
      "source": [
        "# Loading the images, model and mounting it to GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emkh3f1bdkI1"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk0tsjHz_IPS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtNg-Yku8-Zq"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = vgg16(VGG16_Weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adi5K8unAP1x"
      },
      "outputs": [],
      "source": [
        "def normalize_weights(model):\n",
        "    for layer in model.features:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # Calculate the mean activation of the filter\n",
        "            mean_activation = layer.weight.data.mean()\n",
        "            # Scale the weights to achieve mean activation of approximately one\n",
        "            layer.weight.data /= mean_activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWArtqUF9Ccv"
      },
      "outputs": [],
      "source": [
        "# model.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrfZfSOv_DOI"
      },
      "outputs": [],
      "source": [
        "# model.named_children"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hPb13SGUU6A"
      },
      "source": [
        "# Pictures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wnanVdbiH6Z"
      },
      "source": [
        "Select an index for style images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo6w_fdyiM5X"
      },
      "outputs": [],
      "source": [
        "style_image_paths = [\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/TheStaryNight.jpg',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/abstract.jpg',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/hand-drawing.jpg',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/pasted.png',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/PicassoMio.jpg',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/Acrylic-Texture-4.jpg',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/texture.jpeg',\n",
        "    '/content/drive/MyDrive/StyleTransfer/style_images/TheScream.jpg'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9A5KzlJ9EnP"
      },
      "outputs": [],
      "source": [
        "style_pic_path = style_image_paths[7]\n",
        "content_pic_path = '/content/drive/MyDrive/StyleTransfer/content_images/Amsterdam.jpg'\n",
        "new_size = (512, 512)\n",
        "\n",
        "style_pic = (torch.tensor(cv2.cvtColor(cv2.imread(style_pic_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)) / 255.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "content_pic = (torch.tensor(cv2.cvtColor(cv2.imread(content_pic_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)) / 255.0).permute(2, 0, 1).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "style_pic = F.interpolate(style_pic, size= new_size, mode='bilinear', align_corners=False)\n",
        "content_pic = F.interpolate(content_pic, size= new_size, mode='bilinear', align_corners=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12ENL0IY-RzU"
      },
      "outputs": [],
      "source": [
        "plt.imshow(style_pic.squeeze().permute(1, 2, 0).cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsDuYQTHAfEN"
      },
      "outputs": [],
      "source": [
        "plt.imshow(content_pic.squeeze().permute(1, 2, 0).cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr8sgFhrVLQG"
      },
      "source": [
        "# Gram matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC8q9HXVS1TG"
      },
      "outputs": [],
      "source": [
        "class GramMatrix(nn.Module):\n",
        "    def forward(self, x):\n",
        "        batch, channels, height, width = x.size()\n",
        "        features = x.view(batch, channels, height * width)\n",
        "        gram_matrix = torch.bmm(features, features.transpose(1, 2))\n",
        "        return gram_matrix\n",
        "compute_gram_matrix = GramMatrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VlrzMv2UXCe"
      },
      "source": [
        "# Extracting the desired feature maps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHFvaq47KTVV"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, model, num_layers):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.features = nn.Sequential()\n",
        "        i = 0\n",
        "        for name, layer in model.features.named_children():\n",
        "            self.features.add_module(name, layer)\n",
        "            if i == num_layers:\n",
        "                break\n",
        "            i += 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8id5w3OHFjUz"
      },
      "outputs": [],
      "source": [
        "content_layer = 19\n",
        "style_layers = [0, 5, 10, 17, 24]\n",
        "\n",
        "content_feature_extractor = FeatureExtractor(model, content_layer)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    content_features = content_feature_extractor(content_pic)\n",
        "\n",
        "style_gram_matrices = list()\n",
        "for style_layer in style_layers:\n",
        "    style_feature_extractor = FeatureExtractor(model, style_layer)\n",
        "    with torch.no_grad():\n",
        "        style_features = style_feature_extractor(style_pic)\n",
        "        gram_matrix = compute_gram_matrix(style_features)\n",
        "        style_gram_matrices.append(gram_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byqSs-lpdIbo"
      },
      "source": [
        "# New image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlxNZ3HcdNRc"
      },
      "outputs": [],
      "source": [
        "input_image = torch.clone(content_pic).requires_grad_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTHoM6LceEwm"
      },
      "source": [
        "#Loss functions and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZV6k108eJSA"
      },
      "outputs": [],
      "source": [
        "loss_content = nn.MSELoss(reduction= 'sum')\n",
        "loss_style = nn.MSELoss(reduction= 'sum')\n",
        "optimizer = torch.optim.LBFGS(params= [input_image], lr=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouMPRuFe-PSw"
      },
      "outputs": [],
      "source": [
        "def calculate_gram_matrices(input_image, style_layers):\n",
        "    input_gram_matrices= list()\n",
        "    M = list()\n",
        "\n",
        "    for layer_idx in style_layers:\n",
        "        style_feature_extractor = FeatureExtractor(model, layer_idx)\n",
        "        input_style_features = style_feature_extractor(input_image)\n",
        "\n",
        "        gram_matrix = compute_gram_matrix(input_style_features)\n",
        "        input_gram_matrices.append(gram_matrix)\n",
        "        m = input_style_features[0].shape[1] * input_style_features[0].shape[2]\n",
        "        M.append(m)\n",
        "\n",
        "    return input_gram_matrices, M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmiVeYpqeK_2"
      },
      "outputs": [],
      "source": [
        "def calculate_style_loss(input_gram_matrices, style_gram_matrices, W: list, M: list):\n",
        "    assert len(input_gram_matrices) == len(style_gram_matrices) == len(W) == len(M)\n",
        "    L = len(style_gram_matrices)\n",
        "\n",
        "    style_losses = list()\n",
        "\n",
        "    for layer in range(L):\n",
        "        N = input_gram_matrices[layer].shape[0]\n",
        "        style_loss = W[layer] * (loss_style(input_gram_matrices[layer], style_gram_matrices[layer]) / (4 * N**2 * M[layer]**2))\n",
        "        style_losses.append(style_loss)\n",
        "\n",
        "    total_style_loss = sum(style_losses)\n",
        "\n",
        "    return total_style_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TisXNayDEuw"
      },
      "source": [
        "# Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDN_4fqN-M8Y"
      },
      "outputs": [],
      "source": [
        "def closure():\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Calculate content features and Gram matrices within the closure\n",
        "    input_content_features = content_feature_extractor(input_image)\n",
        "    input_gram_matrices, M = calculate_gram_matrices(input_image, style_layers)\n",
        "\n",
        "    # Loss computation\n",
        "    content_loss = loss_content(input_content_features, content_features) / 2\n",
        "    style_loss = calculate_style_loss(input_gram_matrices, style_gram_matrices, W, M)\n",
        "\n",
        "    total_loss = (a * content_loss) + (b * style_loss)\n",
        "    total_loss.backward()\n",
        "\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8atp5UaI-hFS"
      },
      "outputs": [],
      "source": [
        "def generate(steps, W: list, a, b):\n",
        "    for step in steps:\n",
        "        optimizer.step(closure)\n",
        "        print(f'Step number {step}')\n",
        "        # plt.imshow(input_image.squeeze().permute(1, 2, 0).detach().cpu().numpy())\n",
        "        # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fQZZkptXBWn"
      },
      "outputs": [],
      "source": [
        "steps = range(1, 16)\n",
        "W = [(1 / len(style_layers)) for x in range(len(style_layers))]\n",
        "a = 1\n",
        "b = 10000\n",
        "generate(steps, W, a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generated image"
      ],
      "metadata": {
        "id": "pq4w0XPq7H04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(input_image.squeeze().permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OnMZ2euEMi5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_concated = torch.cat([content_pic, style_pic, input_image], dim= 3)"
      ],
      "metadata": {
        "id": "VdI48ry1OB4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images_concated.squeeze().permute(1, 2, 0).detach().cpu().numpy())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hB-GKxgJQiXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the image"
      ],
      "metadata": {
        "id": "OB1bj7j17MBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_to_save = cv2.cvtColor((input_image.squeeze().permute(1, 2, 0).detach().cpu().numpy() * 255.0), cv2.COLOR_RGB2BGR)\n",
        "images_to_save = cv2.cvtColor((images_concated.squeeze().permute(1, 2, 0).detach().cpu().numpy() * 255.0), cv2.COLOR_RGB2BGR)\n",
        "cv2.imwrite('/content/drive/MyDrive/StyleTransfer/Examples/generation.jpg', image_to_save)\n",
        "cv2.imwrite('/content/drive/MyDrive/StyleTransfer/Examples/example.jpg', image_to_save)"
      ],
      "metadata": {
        "id": "6OMrIh6eI6XY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}